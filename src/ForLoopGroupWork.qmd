---
bibliography: bio.bib
csl: harvard-cite-them-right.csl
title: Group Name's Group Project
execute:
  echo: false
format:
  html:
    theme:
      - minty
      - css/web.scss
    code-copy: true
    code-link: true
    toc: true
    toc-title: On this page
    toc-depth: 2
    toc_float:
      collapsed: false
      smooth_scroll: true
  pdf:
    include-in-header:
      text: |
        \addtokomafont{disposition}{\rmfamily}
    mainfont: Spectral
    sansfont: Roboto
    monofont: JetBrainsMono-Regular
    papersize: a4
    geometry:
      - top=25mm
      - left=40mm
      - right=30mm
      - bottom=25mm
      - heightrounded
    toc: false
    number-sections: false
    colorlinks: true
    highlight-style: github
jupyter:
  jupytext:
    text_representation:
      extension: .qmd
      format_name: quarto
      format_version: '1.0'
      jupytext_version: 1.15.2
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

## Declaration of Authorship {.unnumbered .unlisted}

We, [For loop], confirm that the work presented in this assessment is our own. Where information has been derived from other sources, we confirm that this has been indicated in the work. Where a Large Language Model such as ChatGPT has been used we confirm that we have made its contribution to the final submission clear.

Date:

Student Numbers: 

## Brief Group Reflection

| What Went Well | What Was Challenging |
| -------------- | -------------------- |
| A              | B                    |
| C              | D                    |

## Priorities for Feedback

Are there any areas on which you would appreciate more detailed feedback if we're able to offer it?

```{=html}
<style type="text/css">
.duedate {
  border: dotted 2px red; 
  background-color: rgb(255, 235, 235);
  height: 50px;
  line-height: 50px;
  margin-left: 40px;
  margin-right: 40px
  margin-top: 10px;
  margin-bottom: 10px;
  color: rgb(150,100,100);
  text-align: center;
}
</style>
```

{{< pagebreak >}}

# Response to Questions

```{python}
import os
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
```

```{python}
host = 'https://orca.casa.ucl.ac.uk'
path = '~jreades/data'
file = '2022-09-10-listings.csv.gz'
url  = f'{host}/{path}/{file}'
local_url = f'../data/{file}'


if os.path.exists(file):
  #df = pd.read_csv(file, compression='gzip', low_memory=False)
else: 
  #df = pd.read_csv(url, compression='gzip', low_memory=False)
    # 下载该文件到本地 的data文件夹 -- lwx
  #df.to_csv(local_url)
```

## 1. Who collected the data?
The data of listing of Airbnb used in the project is mainly collected by Inside Airbnb (Inside Airbnb 2023) . 

## 2. Why did they collect it?
The goal of IA, a mission-driven activist project, is to create a platform to support the advocacy of policies that protect our cities from the negative effects of short-term rentals and to provide data that quantifies the impact of long-term rentals on housing and residential communities.

## 3. How was the data collected?  
The data used in the analysis were mainly from Inside Airbnb, which is independent. To be more specific, the website is not related to the official Airbnb site and is protected under a Creative Commons CC0 1.0 Universal (CCO 1.0) ‘Public Domain Dedication’ license. The data includes information such as the location, price, number of bedrooms, number of reviews, and other details about the listings. Moreover, the data from Inside Airbnb is typically collected through a combination of web scraping Airbnb's publicly available listings and analyzing this data for insights. Within Airbnb utilizes automated scripts to gather data from the Airbnb website in a methodical manner, with an emphasis on publicly accessible information from different listings.  Data is gathered via scraping and then assembled into a database.  This procedure involves organizing the data so that it can be easily analyzed and refining it by removing duplicates and fixing errors.

## 4. How does the method of collection impact the completeness and/or accuracy of its representation of the process it seeks to study, and what wider issues does this raise?
According to the Airbnb project website (The Data, 2023), the location of a specific Airbnb on a map or in the data is 0 to 450 feet from the actual address because Airbnb listings’ location data is anonymized. As a result, minor errors may occur when plotting the locations of Airbnb. In addition, on the Inside Airbnb website, both booked and unavailable are classified into “unbookable nights” which may affect the calculation of occupancy rate because the availability of Airbnb is used in the algorithm in this study. Furthermore, the information supplied by Inside Airbnb represents a moment in time snapshot of the Airbnb listings which means that it is possible that some of the Airbnb listings in the dataset have been removed or that new listings have been added since the snapshots were taken. For instance, the accuracy of the review data obtained from Inside Airbnb dataset was questioned (Abdulkareem Alsudais, 2021) : The number of reviews associated with each listing in IA rises as a result of these instances of inaccurate data. There are very seldom instances where there are more false reviews associated with a listing than true and accurate reviews.

## 5. What ethical considerations does the use of this data raise? 
Use of Scraped Data:
The ethics of web scraping for data collection are complex. While the data is publicly available, the process of scraping and republishing raises questions about consent and the appropriate use of publicly available information.
Impact on Airbnb Hosts:
Although Inside Airbnb claims that No "private" information is being used. Names, photographs, listings and review details are all publicly displayed on the Airbnb site. However, research conclusions based on crawling information may affect the normal operation of landlords.
Data Accuracy and Misrepresentation:
Inside Airbnb data scraped from websites may not always be accurate or up-to-date. Relying on such data for research or policy-making could lead to decisions based on incomplete or misleading information. 

## 6. With reference to the data (*i.e.* using numbers, figures, maps, and descriptive statistics), what does an analysis of Hosts and Listing types suggest about the nature of Airbnb lets in London? 

::: {.duedate}

### path 1.1 find local owner --diff Hosts
区分 Hosts 房东类型 -- 聚类（中介&当地）
(solution : number of house they have) {cluster 1}

```{python}
import os
import numpy as np
import pandas as pd
import seaborn as sns
```

```{python}
# loading data and data wrangling
## loading data
df = pd.read_csv('../data/listings.csv', low_memory=False)
## data wrangling
### select the column of 'host_id' to make a new dataframe called df1
df1 = df[['host_id']]
num_rows = len(df1)
### creat a new dataframe
# to make a column called ones (I don't like to use groupby().count, as the if i groupby the 'host_id' then the other like 'name' 
# that i select from df will show the count of 'host_id' that could be ambiguous)
### create an array where each element is 1 using np.ones
ones_array = np.ones((num_rows, 1))

### convert the array to a DataFrame
df_ones = pd.DataFrame(ones_array, columns=['ones'])
## make every lines' value of 'one' to be int
df1['ones'] = df_ones['ones'].astype(int)
### convert the values in the 'one' column of each row to integers
grouped_df1 = df1.groupby('host_id').sum()
grouped_df1 = grouped_df1.reset_index().rename(columns={'ones': 'counts'})
```

```{python}
### convert the values in the 'one' column of each row to integers
grouped_df1 = df1.groupby('host_id').sum()
grouped_df1 = grouped_df1.reset_index().rename(columns={'ones': 'counts'})
```

#### K-means聚类

```{python}
# do K-means clustering analysis to find the proper value that could be used to filters out local landlords and companies
## do K-means clustering analysis
from sklearn.cluster import KMeans
the_count = grouped_df1[['counts']]  # Use double brackets to maintain DataFrame format


k_pref = 3  # The number of clusters have chosen
kmeans = KMeans(n_clusters=k_pref,  n_init=31, random_state=42)
kmeans.fit(the_count)

# join the cluter to the origin dataframe
grouped_df1['local_or_company'] = kmeans.labels_
```

```{python}
## Plot the result of clustering
import matplotlib.pyplot as plt
import seaborn as sns

# Set the plotting style
sns.set(style="whitegrid")

# Create a scatter plot where the x-axis is the index, the y-axis is the value of 'calculated_host_listings_count', and the color is determined by cluster labels
plt.figure(figsize=(13, 11))
sns.scatterplot(x=grouped_df1.index, y=grouped_df1['counts'], hue=grouped_df1['local_or_company'], palette='viridis')

# Add chart title and labels
plt.title('K-Means Clustering of the_count')
plt.xlabel('Index')
plt.ylabel('counts')

# Generate the chart
plt.legend(title='Cluster')
plt.show()
```

```{python}
## Define based on facts
# df is my DataFrame
# Replace values in the 'local_or_company' column that are equal to 0 with 1, and replace other values with 0
grouped_df1['local_or_company'] = np.where(grouped_df1['local_or_company'] == 0, 1, 0)

## plot again for checking
# Set the plotting style
sns.set(style="whitegrid")

# Create a scatter plot where the x-axis is the index, the y-axis is the value of 'calculated_host_listings_count', and the color is determined by cluster labels
plt.figure(figsize=(11, 6))
sns.scatterplot(x=grouped_df1.index, y=grouped_df1['counts'], hue=grouped_df1['local_or_company'], palette='viridis')

# Select rows in df that are local landlords, then find the maximum value of the 'calculated_host_listings_count' column from the filtered data. This calculates the maximum number of houses owned by local landlords
# First, select the rows where the value in the 'local_or_company' column is 1, representing local landlords
filtered_rows = grouped_df1[grouped_df1['local_or_company'] == 1]

# Then, find the maximum value in the 'b' column among these rows
max_value_in_local_hosts = filtered_rows['counts'].max()
print(f'Max value in local hosts is: {max_value_in_local_hosts}')

# Add a red dashed line at y=max_value_in_local_hosts, and label it as 'max value of the number of Airbnbs owned by local landlords'
plt.axhline(y=max_value_in_local_hosts, color='red', linestyle='--', label=max_value_in_local_hosts)

# Add chart title and labels
plt.title('K-Means Clustering of the count that is Airbnbs owned by local landlords and the companies')
plt.xlabel('id')
plt.ylabel('the numeber of Airbnbs')

# Display the legend and the chart
plt.legend(title='The type of hosts')
plt.show()
```

```{python}
## better know the characteristics of the data and find that maybe Gaussian Mixture Model fit the data
# check data using Kernel Density Estimate (KDE) plot
grouped_df1.counts.plot.kde(xlim=(0,20)); #kernel density estimate plot
```

```{python}
# do clustering analysis using Gaussian Mixture Model 
## setting different n_components value sizes to produce different clusters analyses rationality
from sklearn.mixture import GaussianMixture

# grouped_df1 is your DataFrame, containing the column 'counts'
# Convert the 'counts' column into a format suitable for clustering (two-dimensional array).
X = grouped_df1[['counts']].values

# Create a GMM (Gaussian Mixture Model) instance with the number of clusters set to 2
gmm = GaussianMixture(n_components=2, random_state=0)

# Fit and predict the data
grouped_df1['cluster'] = gmm.fit_predict(X)

# View the clustering results
print(grouped_df1.head())

# Check the statistics for each cluster
print(grouped_df1.groupby('cluster')['counts'].describe())
```

```{python}
## maybe 595 is the outlier, so drop it and try different n_components value sizes
# Find the index of the maximum value in the 'cluster' column
max_index = grouped_df1['counts'].idxmax()

# Delete this row
grouped_df1 = grouped_df1.drop(max_index)
```

```{python}
## try the GMM to do clustering again
# Convert the 'counts' column into a format suitable for clustering (two-dimensional array)
X = grouped_df1[['counts']].values

# Create a GMM (Gaussian Mixture Model) instance with the number of clusters set to 2
gmm = GaussianMixture(n_components=3, random_state=0)

# Fit the data and make predictions
grouped_df1['local_or_company'] = gmm.fit_predict(X)

# View the clustering results
print(grouped_df1.head())

# Check the statistics for each cluster
print(grouped_df1.groupby('local_or_company')['counts'].describe())
```

```{python}
## Define based on facts
# Replace values in the 'local_or_company' column that are equal to 0 and 2 with 1, and replace other values with 0
grouped_df1['local_or_company'] = np.where(grouped_df1['local_or_company'] == 0|2, 1, 0)
```

```{python}
## plot again for visulisation and get the max acount of Airbnbs owned by local landlords
# Set the plotting style
sns.set(style="whitegrid")

# Create a scatter plot where the x-axis is the index, the y-axis is the value of 'calculated_host_listings_count,' and the color is determined by cluster labels
plt.figure(figsize=(11, 6))
sns.scatterplot(x=grouped_df1.index, y=grouped_df1['counts'], hue=grouped_df1['local_or_company'], palette='viridis')

# Select rows in df that are local landlords, then find the maximum value of the 'calculated_host_listings_count' column from the filtered data. This calculates the maximum number of houses owned by local landlords
# First, select the rows where the value in the 'local_or_company' column is 1, representing local landlords
filtered_rows = grouped_df1[grouped_df1['local_or_company'] == 1]

# Then, find the maximum value in the 'b' column among these rows
max_value_in_local_hosts_GMM = filtered_rows['counts'].max()
print(f'Max value in local hosts is: {max_value_in_local_hosts_GMM}')

# Add a red dashed line at y=max_value_in_local_hosts, and label it as 'max value of the number of Airbnbs owned by local landlords
plt.axhline(y=max_value_in_local_hosts_GMM, color='red', linestyle='--', label=max_value_in_local_hosts_GMM)

# Add chart title and labels
plt.title('The Clustering of Gaussian Mixture Model of the count that is Airbnbs owned by local landlords and the companies')
plt.xlabel('id')
plt.ylabel('the numeber of Airbnbs')

# Display the legend and the chart
plt.legend(title='The type of hosts')
plt.show()
```

```{python}
# Process the original dataset df by selecting rows where the 'calculated_host_listings_count' column is less than or equal to the max_value_in_local_hosts value and set a new 'local_or_company' column with 1 (1 represents local landlords, 0 represents companies)
df['local_or_company'] = np.where(df['calculated_host_listings_count'] <= max_value_in_local_hosts_GMM, 1, 0)
df.head(14)
```

*  (optional) Listing 房源类型 -- 描述性统计--（暂定）
  
  图表

* airbnb 的空间分布 （聚集或分散）


( 15 points; Answer due {{< var assess.group-date >}} )

:::

## 7. Drawing on your previous answers, and supporting your response with evidence (e.g. figures, maps, and statistical analysis/models), how *could* this data set be used to inform the regulation of Short-Term Lets (STL) in London? 

::: {.duedate}

### 计算当地经济  path 1 economy 
(定量计算当地经济--local economy mutiplier )
rental per night (column) --{algorithm 1}--> diff owner's income --{algorithm 2}--> economy by areas 

#### 计算
* 房东收入 estimated_annual_income
* 游客支出 tourists_expenditure

```{python}
# Assuming DataFrame is named 'df'
# Replace 'df' with the actual name of your DataFrame variable

import re

# Setting the model parameters
review_rate = 0.5
average_days = 3

# Selecting listings where availability_365 is greater than 0
df_geq_0 = df[df.availability_365 > 0]

# Calculating estimated annual nights
df.loc[df.availability_365 > 0, 'estimated_annual_nights'] = (
    (df_geq_0.reviews_per_month / review_rate * average_days * 12) / 365
) * df_geq_0.availability_365

# Setting estimated annual income to 0 for listings with availability_365 equals 0
df.loc[df.availability_365 == 0, 'estimated_annual_income'] = 0

# Calculating estimated annual income
df.loc[df.availability_365 > 0, 'estimated_annual_income'] = df['estimated_annual_nights'] * df_geq_0.price

# Filling any missing values in both new columns with 0
df[['estimated_annual_nights', 'estimated_annual_income']] = df[['estimated_annual_nights', 'estimated_annual_income']].fillna(0)

# Extract the number of beds from the 'name' column.
def extract_beds(name):
    match = re.search(r'(\d+)\s+beds?', name)
    return int(match.group(1)) if match else 0

df['number_of_beds'] = df['name'].apply(extract_beds)

# Calculate 'tourists_expenditure'
df['tourists_expenditure'] = df['number_of_beds'] * df['estimated_annual_nights'] * 100
```

#### 模型 local economic multiplier -- LM3(London)

```{python}
# applying local economic multiplier
# 模型参数
coef1 = 1.76
coef2 = 0.36

# local landlords using 1.76, real estate company landlord using 0.36
df.loc[df.local_or_company==1,'value_to_local_economy']=df.loc[df.local_or_company==1,'estimated_annual_income']*coef1
df.loc[df.local_or_company==0,'value_to_local_economy']=df.loc[df.local_or_company==0,'estimated_annual_income']*coef2

# Update 'value_to_local_economy' by adding the product of 'tourists_expenditure' and coef1(1.76)
df['value_to_local_economy'] += df['tourists_expenditure'] * coef1

df = df.groupby('neighbourhood')['value_to_local_economy'].sum().reset_index(name='local_economy_contribution_from_airbnb')

df['local_economy_contribution_from_airbnb'] = df['local_economy_contribution_from_airbnb'].round(2)

# Generate a new CSV file to display the local economy contribution from Airbnb tourism for each borough.
# df.to_csv('local_economy_contribution_from_airbnb_tourism.csv', index=False)
```

```{python}
# df.head(10)
# dataframe -- local_economy_contribution_from_airbnb + neighbourhood
```

( 45 points; Answer due {{< var assess.group-date >}} )

### 旅游区聚类  path 2 tourism areas
number of tourism destination + boundary in london --{cluster 2}--> by number of tourism destination by per areas --> 
（根据每个旅游区的旅游景点数量，划分旅游区和非旅游区）

:::

```{python}
# package
import json
import pandas as pd
from shapely.geometry import Point
import geopandas as gpd
```

#### 数据读取

```{python}
# Shapefile 路径
shapefile_path = '../data/London_Borough_Excluding_MHW/London_Borough_Excluding_MHW.shp'

# 数据读取
file_path = '../data/export.geojson'
# 读取 GeoJSON 文件
with open(file_path, 'r', encoding='utf-8') as file:
    geojson_data = json.load(file)
# 准备一个列表来存储点坐标
point_coordinates = []

# 遍历特征集合中的每个特征--并用列表进行存储
for feature in geojson_data.get('features', []):
    # 检查几何类型是否为点
    if feature['geometry']['type'] == 'Point':
        # 提取坐标
        coordinates = feature['geometry']['coordinates']
        point_coordinates.append(coordinates)
```

```{python}
points = [Point(x, y) for x, y in point_coordinates]

# 创建一个 DataFrame
df = pd.DataFrame(point_coordinates, columns=['Longitude', 'Latitude'])

# 使用 DataFrame 和 Points 创建 GeoDataFrame
attraction_gdf = gpd.GeoDataFrame(df, geometry=points)
```

```{python}
# 加载 Shapefile
borough = gpd.read_file(shapefile_path)
attraction_gdf.set_crs(epsg=4326, inplace=True)
# 确保两个 GeoDataFrame 使用相同的坐标参考系统
attraction_gdf = attraction_gdf.to_crs(borough.crs)
```

#### 景点数量统计
通过空间连接，统计每个borough内部的景点数量

```{python}
# 景点数量统计
# 使用空间连接来统计每个多边形包含的点数
point_in_polygon = gpd.sjoin(attraction_gdf, borough, how='inner', op='within')
# 计算每个多边形包含的点数
count_points_in_polygon = point_in_polygon.groupby('GSS_CODE').size().reset_index(name='point_count')

# 执行空间连接
point_in_polygon = gpd.sjoin(attraction_gdf, borough, how='inner', op='within')

# 对连接结果进行分组统计
count_points = point_in_polygon.groupby('GSS_CODE').size()

# 将计数结果转换为 DataFrame，并重置索引
count_points_df = count_points.reset_index(name='point_count')

# 保证所有多边形都包含在统计结果中
count_points_in_polygon = borough.merge(count_points_df, on='GSS_CODE', how='left')

# 将未包含点的多边形的点数设为零
count_points_in_polygon['point_count'].fillna(0, inplace=True)

# 显示结果
# print(count_points_in_polygon[['GSS_CODE', 'point_count']])
```

####  Visualization
展示每个borough内部的景点数量

```{python}
# 假设 count_points_in_polygon 是您的 GeoDataFrame
# count_points_in_polygon 应该包含一个名为 'point_count' 的列
# 使用 'point_count' 列的值来设置颜色映射
# 'plasma' 是一个由浅至深的颜色映射，您可以根据需要选择其他颜色映射
count_points_in_polygon.plot(column='point_count', cmap='plasma', legend=True, figsize=(10, 6))

# 添加标题和标签（可选）
plt.title('Visualization by Point Count')
plt.xlabel('Longitude')
plt.ylabel('Latitude')

# 显示图表
plt.show()
```

```{python}
# 提取 point_count 作为聚类的特征
X = count_points_in_polygon[['point_count']].values

# 选择聚类的数量
n_clusters = 3

# 创建 KMeans 实例并拟合数据
kmeans = KMeans(n_clusters=n_clusters)
kmeans.fit(X)

# 将聚类标签添加到原始 DataFrame
count_points_in_polygon['cluster'] = kmeans.labels_

# 可视化聚类结果（如果适用）
plt.scatter(count_points_in_polygon['GSS_CODE'], count_points_in_polygon['point_count'], c=count_points_in_polygon['cluster'])
plt.xlabel('GSS_CODE')
plt.ylabel('Point Count')
plt.title('Cluster Analysis of Point Counts in Polygons')
plt.show()

# 查看聚类结果
# print(count_points_in_polygon)
```

#### 聚类 区分 旅游区 & 非旅游区
按照每个borough的旅游景点数量，划分旅游区和非旅游区

```{python}
non_tourism_borough = count_points_in_polygon[count_points_in_polygon['cluster'] == 3]
# 假设 count_points_in_polygon 是您的 GeoDataFrame
# count_points_in_polygon 应该包含一个名为 'cluster' 的列

# 设置颜色：cluster 等于 3 为一种颜色，其余为另一种颜色
colors = count_points_in_polygon['cluster'].map(lambda x: 'silver' if x == 1 else 'grey')

# 绘制 GeoDataFrame
count_points_in_polygon.plot(color=colors, figsize=(10, 6))

# 添加图例（可选）
plt.scatter([], [], color='silver', label='non tourism borough')
plt.scatter([], [], color='grey', label='tourism borough')
plt.legend(title='Clusters')

# 显示图表
plt.show()
```

## Sustainable Authorship Tools

Your QMD file should automatically download your BibTeX file. We will then re-run the QMD file to generate the output successfully.

Written in Markdown and generated from [Quarto](https://quarto.org/). Fonts used: [Spectral](https://fonts.google.com/specimen/Spectral) (mainfont), [Roboto](https://fonts.google.com/specimen/Roboto) (<span style="font-family:Sans-Serif;">sansfont</span>) and [JetBrains Mono](https://fonts.google.com/specimen/JetBrains%20Mono) (`monofont`). 

## References
