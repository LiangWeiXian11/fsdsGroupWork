---
bibliography: bio.bib
csl: harvard-cite-them-right.csl
title: Group Name's Group Project
execute:
  echo: false
format:
  html:
    theme:
      - minty
      - css/web.scss
    code-copy: true
    code-link: true
    toc: true
    toc-title: On this page
    toc-depth: 2
    toc_float:
      collapsed: false
      smooth_scroll: true
  pdf:
    include-in-header:
      text: |
        \addtokomafont{disposition}{\rmfamily}
    mainfont: Spectral
    sansfont: Roboto
    monofont: JetBrainsMono-Regular
    papersize: a4
    geometry:
      - top=25mm
      - left=40mm
      - right=30mm
      - bottom=25mm
      - heightrounded
    toc: false
    number-sections: false
    colorlinks: true
    highlight-style: github
jupyter:
  jupytext:
    text_representation:
      extension: .qmd
      format_name: quarto
      format_version: '1.0'
      jupytext_version: 1.15.2
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

## Declaration of Authorship {.unnumbered .unlisted}

We, [For loop], confirm that the work presented in this assessment is our own. Where information has been derived from other sources, we confirm that this has been indicated in the work. Where a Large Language Model such as ChatGPT has been used we confirm that we have made its contribution to the final submission clear.

Date:

Student Numbers: 

## Brief Group Reflection

| What Went Well | What Was Challenging |
| -------------- | -------------------- |
| A              | B                    |
| C              | D                    |

## Priorities for Feedback

Are there any areas on which you would appreciate more detailed feedback if we're able to offer it?

```{=html}
<style type="text/css">
.duedate {
  border: dotted 2px red; 
  background-color: rgb(255, 235, 235);
  height: 50px;
  line-height: 50px;
  margin-left: 40px;
  margin-right: 40px
  margin-top: 10px;
  margin-bottom: 10px;
  color: rgb(150,100,100);
  text-align: center;
}
</style>
```

{{< pagebreak >}}

# Response to Questions

```{python}
import os
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
```

```{python}
host = 'https://orca.casa.ucl.ac.uk'
path = '~jreades/data'
file = '2022-09-10-listings.csv.gz'
url  = f'{host}/{path}/{file}'
local_url = f'../data/{file}'


if os.path.exists(file):
  df = pd.read_csv(file, compression='gzip', low_memory=False)
else: 
  df = pd.read_csv(url, compression='gzip', low_memory=False)
    # 下载该文件到本地 的data文件夹 -- lwx
  df.to_csv(local_url)
```

## 1. Who collected the data?
Firstly, the data of listing data of Airbnb used in the project is mainly collected by Inside Airbnb (). Secondly, the shapefile of the London boroughs is determined by the government of UK. In addition, the tourist attractions in London are collected by Open Street Map (OSM).
## 2. Why did they collect it?

::: {.duedate}

* Use the distribution of scenic spots and wards boundary to select non-tourism-oriented wards


( 4 points; Answer due Week 7 )

:::

```{python}
print(f"Data frame is {df.shape[0]:,} x {df.shape[1]:,}")
```

```{python}
ax = df.host_listings_count.plot.hist(bins=50);
ax.set_xlim([0,500]);
```

## 3. How was the data collected?  
Inside Airbnb:
The data used in the analysis were mainly from Inside Airbnb, which is independent. To be more specific, the website is not related to the official Airbnb site and is protected under a Creative Commons CC0 1.0 Universal (CCO 1.0) ‘Public Domain Dedication’ license. The data includes information such as the location, price, number of bedrooms, number of reviews, and other details about the listings. Moreover, the data from Inside Airbnb is typically collected through a combination of web scraping Airbnb's publicly available listings and analyzing this data for insights. Within Airbnb utilizes automated scripts to gather data from the Airbnb website in a methodical manner, with an emphasis on publicly accessible information from different listings.  Data is gathered via scraping and then assembled into a database.  This procedure involves organizing the data so that it can be easily analyzed and refining it by removing duplicates and fixing errors.
Points of Interest (PoI) data from Open Street Map(OSM):
The OSM POI points for London attractions were used in this study. Data for OpenStreetMap (OSM) is gathered using a range of techniques, mostly by volunteers in the community. These techniques include working with organizations, importing data, using GPS and manual surveys, and using local expertise. The data structure is made up of point data with attributes, such as geographic information, assigned to each data point.
London Borough Boundaries:
Official Ordnance Survey polygons showing Borough boundaries and reference code. Ordnance Survey is the British national mapping agency that provides geographic data for government, business, and individuals. Which is accessible from the London Datastore.
## 4. How does the method of collection impact the completeness and/or accuracy of its representation of the process it seeks to study, and what wider issues does this raise?

## 5. What ethical considerations does the use of this data raise? 
Use of Scraped Data:
The ethics of web scraping for data collection are complex. While the data is publicly available, the process of scraping and republishing raises questions about consent and the appropriate use of publicly available information.
Impact on Airbnb Hosts:
Although Inside Airbnb claims that No "private" information is being used. Names, photographs, listings and review details are all publicly displayed on the Airbnb site. However, research conclusions based on crawling information may affect the normal operation of landlords.
Data Accuracy and Misrepresentation:
Inside Airbnb data scraped from websites may not always be accurate or up-to-date. Relying on such data for research or policy-making could lead to decisions based on incomplete or misleading information. 
OSM relies on a global network of volunteers to contribute and edit map data. While this decentralized approach ensures the broad coverage of OSM data, it also introduces challenges in ensuring data accuracy and consistency. Volunteers may not have access to all the necessary information or may misinterpret existing data, leading to inaccuracies in map representations. Moreover, the lack of clear guidelines for data contribution and review processes can exacerbate the issue of data bias.
Limitations:
OSM statistical method will affect the comprehensiveness and accuracy of the data to some extent, for example, it does not deeply classify the types and levels of attractions, As a result, some parks that are tourist destinations are not counted among the attractions. To reduce this impact, we checked popular attractions data with London Tourism to ensure that major attractions were taken into account.
Airbnb listings in 2022 have dropped 21% since 2020, which represents 17,000 listings.
The borough with the highest occupancy rate is Hillingdon, where the average Airbnb listing is taken around 17% of the year, probably as a result of its closeness to Heathrow Airport.
Location information for listings are anonymized by Airbnb. In practice, this means the location for a listing on the map, or in the data will be from 0-450 feet (150 metres) of the actual address. Listings in the same building are anonymized by Airbnb individually, and therefore may appear "scattered" in the area surrounding the actual address.

## 6. With reference to the data (*i.e.* using numbers, figures, maps, and descriptive statistics), what does an analysis of Hosts and Listing types suggest about the nature of Airbnb lets in London? 

::: {.duedate}

* Hosts 房东类型 -- 聚类（中介&当地）

  地图
### path 1.1 find local owner --diff Hosts
(possible solution : number of house they have) {cluster 1}

*  (optional) Listing 房源类型 -- 描述性统计--（暂定）
  
  图表

* airbnb 的空间分布 （聚集或分散）


( 15 points; Answer due {{< var assess.group-date >}} )

:::

## 7. Drawing on your previous answers, and supporting your response with evidence (e.g. figures, maps, and statistical analysis/models), how *could* this data set be used to inform the regulation of Short-Term Lets (STL) in London? 

::: {.duedate}

### 计算当地经济  path 1 economy 
(定量计算当地经济--local economy mutiplier )
rental per night (column) --{algorithm 1}--> diff owner's income --{algorithm 2}--> economy by areas 

( 45 points; Answer due {{< var assess.group-date >}} )


### 旅游区聚类  path 2 tourism areas
number of tourism destination + boundary in london --{cluster 2}--> by number of tourism destination by per areas --> 
（根据每个旅游区的旅游景点数量，划分旅游区和非旅游区）

:::

```{python}
# package
import json
import pandas as pd
from shapely.geometry import Point
import geopandas as gpd
```

#### 数据读取

```{python}
# Shapefile 路径
shapefile_path = '../data/London_Borough_Excluding_MHW/London_Borough_Excluding_MHW.shp'

# 数据读取
file_path = '../data/export.geojson'
# 读取 GeoJSON 文件
with open(file_path, 'r', encoding='utf-8') as file:
    geojson_data = json.load(file)
# 准备一个列表来存储点坐标
point_coordinates = []

# 遍历特征集合中的每个特征--并用列表进行存储
for feature in geojson_data.get('features', []):
    # 检查几何类型是否为点
    if feature['geometry']['type'] == 'Point':
        # 提取坐标
        coordinates = feature['geometry']['coordinates']
        point_coordinates.append(coordinates)
```

```{python}
points = [Point(x, y) for x, y in point_coordinates]

# 创建一个 DataFrame
df = pd.DataFrame(point_coordinates, columns=['Longitude', 'Latitude'])

# 使用 DataFrame 和 Points 创建 GeoDataFrame
attraction_gdf = gpd.GeoDataFrame(df, geometry=points)
```

```{python}
# 加载 Shapefile
borough = gpd.read_file(shapefile_path)
attraction_gdf.set_crs(epsg=4326, inplace=True)
# 确保两个 GeoDataFrame 使用相同的坐标参考系统
attraction_gdf = attraction_gdf.to_crs(borough.crs)
```

#### 景点数量统计
通过空间连接，统计每个borough内部的景点数量

```{python}
# 景点数量统计
# 使用空间连接来统计每个多边形包含的点数
point_in_polygon = gpd.sjoin(attraction_gdf, borough, how='inner', op='within')
# 计算每个多边形包含的点数
count_points_in_polygon = point_in_polygon.groupby('GSS_CODE').size().reset_index(name='point_count')

# 执行空间连接
point_in_polygon = gpd.sjoin(attraction_gdf, borough, how='inner', op='within')

# 对连接结果进行分组统计
count_points = point_in_polygon.groupby('GSS_CODE').size()

# 将计数结果转换为 DataFrame，并重置索引
count_points_df = count_points.reset_index(name='point_count')

# 保证所有多边形都包含在统计结果中
count_points_in_polygon = borough.merge(count_points_df, on='GSS_CODE', how='left')

# 将未包含点的多边形的点数设为零
count_points_in_polygon['point_count'].fillna(0, inplace=True)

# 显示结果
print(count_points_in_polygon[['GSS_CODE', 'point_count']])
```

####  Visualization
展示每个borough内部的景点数量

```{python}
# 假设 count_points_in_polygon 是您的 GeoDataFrame
# count_points_in_polygon 应该包含一个名为 'point_count' 的列
# 使用 'point_count' 列的值来设置颜色映射
# 'plasma' 是一个由浅至深的颜色映射，您可以根据需要选择其他颜色映射
count_points_in_polygon.plot(column='point_count', cmap='plasma', legend=True, figsize=(10, 6))

# 添加标题和标签（可选）
plt.title('Visualization by Point Count')
plt.xlabel('Longitude')
plt.ylabel('Latitude')

# 显示图表
plt.show()
```

```{python}
# 提取 point_count 作为聚类的特征
X = count_points_in_polygon[['point_count']].values

# 选择聚类的数量
n_clusters = 3

# 创建 KMeans 实例并拟合数据
kmeans = KMeans(n_clusters=n_clusters)
kmeans.fit(X)

# 将聚类标签添加到原始 DataFrame
count_points_in_polygon['cluster'] = kmeans.labels_

# 可视化聚类结果（如果适用）
plt.scatter(count_points_in_polygon['GSS_CODE'], count_points_in_polygon['point_count'], c=count_points_in_polygon['cluster'])
plt.xlabel('GSS_CODE')
plt.ylabel('Point Count')
plt.title('Cluster Analysis of Point Counts in Polygons')
plt.show()

# 查看聚类结果
print(count_points_in_polygon)
```

#### 聚类 区分 旅游区 & 非旅游区
按照每个borough的旅游景点数量，划分旅游区和非旅游区

```{python}
non_tourism_borough = count_points_in_polygon[count_points_in_polygon['cluster'] == 3]
# 假设 count_points_in_polygon 是您的 GeoDataFrame
# count_points_in_polygon 应该包含一个名为 'cluster' 的列

# 设置颜色：cluster 等于 3 为一种颜色，其余为另一种颜色
colors = count_points_in_polygon['cluster'].map(lambda x: 'silver' if x == 1 else 'grey')

# 绘制 GeoDataFrame
count_points_in_polygon.plot(color=colors, figsize=(10, 6))

# 添加图例（可选）
plt.scatter([], [], color='silver', label='non tourism borough')
plt.scatter([], [], color='grey', label='tourism borough')
plt.legend(title='Clusters')

# 显示图表
plt.show()
```

## Sustainable Authorship Tools

Your QMD file should automatically download your BibTeX file. We will then re-run the QMD file to generate the output successfully.

Written in Markdown and generated from [Quarto](https://quarto.org/). Fonts used: [Spectral](https://fonts.google.com/specimen/Spectral) (mainfont), [Roboto](https://fonts.google.com/specimen/Roboto) (<span style="font-family:Sans-Serif;">sansfont</span>) and [JetBrains Mono](https://fonts.google.com/specimen/JetBrains%20Mono) (`monofont`). 

## References
