---
bibliography: bio.bib
csl: harvard-cite-them-right.csl
title: Group Name's Group Project
execute:
  echo: false
format:
  html:
    theme:
      - minty
      - css/web.scss
    code-copy: true
    code-link: true
    toc: true
    toc-title: On this page
    toc-depth: 2
    toc_float:
      collapsed: false
      smooth_scroll: true
  pdf:
    include-in-header:
      text: |
        \addtokomafont{disposition}{\rmfamily}
    mainfont: Spectral
    sansfont: Roboto
    monofont: JetBrainsMono-Regular
    papersize: a4
    geometry:
      - top=25mm
      - left=40mm
      - right=30mm
      - bottom=25mm
      - heightrounded
    toc: false
    number-sections: false
    colorlinks: true
    highlight-style: github
jupyter:
  jupytext:
    text_representation:
      extension: .qmd
      format_name: quarto
      format_version: '1.0'
      jupytext_version: 1.15.2
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

## Declaration of Authorship {.unnumbered .unlisted}

We, [For loop], confirm that the work presented in this assessment is our own. Where information has been derived from other sources, we confirm that this has been indicated in the work. Where a Large Language Model such as ChatGPT has been used we confirm that we have made its contribution to the final submission clear.

Date: 2023/12/19

Student Numbers: 23086875, 23191686, 23149976, 22084169, 23130396

## Brief Group Reflection

| What Went Well | What Was Challenging |
| -------------- | -------------------- |
| A              | B                    |
| C              | D                    |

## Priorities for Feedback

Are there any areas on which you would appreciate more detailed feedback if we're able to offer it?

```{=html}
<style type="text/css">
.duedate {
  border: dotted 2px red; 
  background-color: rgb(255, 235, 235);
  height: 50px;
  line-height: 50px;
  margin-left: 40px;
  margin-right: 40px
  margin-top: 10px;
  margin-bottom: 10px;
  color: rgb(150,100,100);
  text-align: center;
}
</style>
```

{{< pagebreak >}}

# Response to Questions

## 1. Who collected the data?
The data of listing of Airbnb used in the project is mainly collected by Inside Airbnb (Inside Airbnb 2023)   <sup>1</sup>. 

## 2. Why did they collect it?
The goal of Inside Airbnb, a mission-driven activist project, is to create a platform to support the advocacy of policies that protect our cities from the negative effects of short-term rentals and to provide data that quantifies the impact of long-term rentals on housing and residential communities<sup>1</sup>.

## 3. How was the data collected?  
The data used in the analysis were mainly from Inside Airbnb, which is independent. To be more specific, the website is not related to the official Airbnb site and is protected under a Creative Commons CC0 1.0 Universal (CCO 1.0) ‘Public Domain Dedication’ license. The data includes information such as the location, price, number of bedrooms, number of reviews, and other details about the listings. Moreover, the data from Inside Airbnb is typically collected through a combination of web scraping Airbnb's publicly available listings and analyzing this data for insights. Within Airbnb utilizes automated scripts to gather data from the Airbnb website in a methodical manner, with an emphasis on publicly accessible information from different listings.  Data is gathered via scraping and then assembled into a database.  This procedure involves organizing the data so that it can be easily analyzed and refining it by removing duplicates and fixing errors.

## 4. How does the method of collection impact the completeness and/or accuracy of its representation of the process it seeks to study, and what wider issues does this raise?
According to the Airbnb project website (The Data, 2023)<sup>1</sup>, the location of a specific Airbnb on a map or in the data is 0 to 450 feet from the actual address because Airbnb listings’ location data is anonymized. As a result, minor errors may occur when plotting the locations of Airbnb. In addition, on the Inside Airbnb website, both booked and unavailable are classified into “unbookable nights” which may affect the calculation of occupancy rate because the availability of Airbnb is used in the algorithm in this study. Furthermore, the information supplied by Inside Airbnb represents a moment in time snapshot of the Airbnb listings which means that it is possible that some of the Airbnb listings in the dataset have been removed or that new listings have been added since the snapshots were taken. For instance, the accuracy of the review data obtained from Inside Airbnb dataset was questioned (Abdulkareem Alsudais, 2021)<sup>3</sup>: The number of reviews associated with each listing in IA rises as a result of these instances of inaccurate data. There are very seldom instances where there are more false reviews associated with a listing than true and accurate reviews.

## 5. What ethical considerations does the use of this data raise? 
Use of Scraped Data:
The ethics of web scraping for data collection are complex. While the data is publicly available, the process of scraping and republishing raises questions about consent and the appropriate use of publicly available information.

Impact on Airbnb Hosts:
Although Inside Airbnb claims that No "private" information is being used. Names, photographs, listings and review details are all publicly displayed on the Airbnb site. However, research conclusions based on crawling information may affect the normal operation of landlords.

Data Accuracy and Misrepresentation:
Inside Airbnb data scraped from websites may not always be accurate or up-to-date. Relying on such data for research or policy-making could lead to decisions based on incomplete or misleading information. 

## 6. With reference to the data (*i.e.* using numbers, figures, maps, and descriptive statistics), what does an analysis of Hosts and Listing types suggest about the nature of Airbnb lets in London? 
### 6.1 Identifying types of hosts
In order to classified whether the host of Airbnb is local or not, a cluster analyse is used in the following.

```{python}
import os
import numpy as np
import pandas as pd
import seaborn as sns
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans

# loading data and data wrangling
## loading data
df = pd.read_csv('../data/listings.csv', low_memory=False)
## data wrangling
### select the column of 'host_id' to make a new dataframe called df1
df1 = df[['host_id']]
num_rows = len(df1)
### creat a new dataframe
# to make a column called ones (I don't like to use groupby().count, as the if i groupby the 'host_id' then the other like 'name' 
# that i select from df will show the count of 'host_id' that could be ambiguous)
### create an array where each element is 1 using np.ones
ones_array = np.ones((num_rows, 1))

### convert the array to a DataFrame
df_ones = pd.DataFrame(ones_array, columns=['ones'])
## make every lines' value of 'one' to be int
df1['ones'] = df_ones['ones'].astype(int)
### convert the values in the 'one' column of each row to integers
grouped_df1 = df1.groupby('host_id').sum()
grouped_df1 = grouped_df1.reset_index().rename(columns={'ones': 'counts'})
```

#### 6.2 Cluster Preprocessing - Gaussian Mixture Model

```{python}
## better know the characteristics of the data and find that maybe Gaussian Mixture Model fit the data
# check data using Kernel Density Estimate (KDE) plot

# do clustering analysis using Gaussian Mixture Model 
## setting different n_components value sizes to produce different clusters analyses rationality
from sklearn.mixture import GaussianMixture

# grouped_df1 is your DataFrame, containing the column 'counts'
# Convert the 'counts' column into a format suitable for clustering (two-dimensional array).
X = grouped_df1[['counts']].values

# Create a GMM (Gaussian Mixture Model) instance with the number of clusters set to 2
gmm = GaussianMixture(n_components=2, random_state=0)

# Fit and predict the data
grouped_df1['cluster'] = gmm.fit_predict(X)

# View the clustering results
# print(grouped_df1.head())

# Check the statistics for each cluster
# print(grouped_df1.groupby('cluster')['counts'].describe())

## maybe 595 is the outlier, so drop it and try different n_components value sizes
# Find the index of the maximum value in the 'cluster' column
max_index = grouped_df1['counts'].idxmax()

# Delete this row
grouped_df1 = grouped_df1.drop(max_index)

## try the GMM to do clustering again
# Convert the 'counts' column into a format suitable for clustering (two-dimensional array)
X = grouped_df1[['counts']].values

# Create a GMM (Gaussian Mixture Model) instance with the number of clusters set to 2
gmm = GaussianMixture(n_components=3, random_state=0)

# Fit the data and make predictions
grouped_df1['local_or_company'] = gmm.fit_predict(X)

# View the clustering results
# print(grouped_df1.head())

# Check the statistics for each cluster
# print(grouped_df1.groupby('local_or_company')['counts'].describe())
```

### 6.3 Cluster Results Presentation

```{python}
## Define based on facts
# Replace values in the 'local_or_company' column that are equal to 0 and 2 with 1, and replace other values with 0
grouped_df1['local_or_company'] = np.where(grouped_df1['local_or_company'] == 0|2, 1, 0)

## plot again for visulisation and get the max acount of Airbnbs owned by local landlords
# Set the plotting style
sns.set(style="whitegrid")

# Create a scatter plot where the x-axis is the index, the y-axis is the value of 'calculated_host_listings_count,' and the color is determined by cluster labels
plt.figure(figsize=(11, 6))
sns.scatterplot(x=grouped_df1.index, y=grouped_df1['counts'], hue=grouped_df1['local_or_company'], palette='viridis')

# Select rows in df that are local landlords, then find the maximum value of the 'calculated_host_listings_count' column from the filtered data. This calculates the maximum number of houses owned by local landlords
# First, select the rows where the value in the 'local_or_company' column is 1, representing local landlords
filtered_rows = grouped_df1[grouped_df1['local_or_company'] == 1]

# Then, find the maximum value in the 'b' column among these rows
max_value_in_local_hosts_GMM = filtered_rows['counts'].max()
print(f'Max value in local hosts is: {max_value_in_local_hosts_GMM}')

# Add a red dashed line at y=max_value_in_local_hosts, and label it as 'max value of the number of Airbnbs owned by local landlords
plt.axhline(y=max_value_in_local_hosts_GMM, color='red', linestyle='--', label=max_value_in_local_hosts_GMM)

# Add chart title and labels
plt.title('The Clustering of Gaussian Mixture Model of the count that is Airbnbs owned by local landlords and the companies')
plt.xlabel('id')
plt.ylabel('the numeber of Airbnbs')

# Display the legend and the chart
plt.legend(title='The type of hosts')
plt.show()

# Process the original dataset df by selecting rows where the 'calculated_host_listings_count' column is less than or equal to the max_value_in_local_hosts value and set a new 'local_or_company' column with 1 (1 represents local landlords, 0 represents companies)
df['local_or_company'] = np.where(df['calculated_host_listings_count'] <= max_value_in_local_hosts_GMM, 1, 0)
# df.head(5)
```

### 6.4 Visualizing Spatial Distribution of Airbnb Listings

```{python}
import geopandas as gpd
# 读取borough数据
# 替换这里的路径为你解压后.shp文件的实际路径
# 读取.shp文件
shapefile_path = '../data/London_Borough_Excluding_MHW/London_Borough_Excluding_MHW.shp'
gdf_bor = gpd.read_file(shapefile_path)

# 查看数据
#print(gdf_bor.head())

# Check the Projection and use the EPSG:27700 projected CRS
#print(gdf_bor.crs)
# gdf.to_crs('epsg:27700')

# convert lat/long to GeoSeries (converted the InsideAirbnb data to a GeoDataFrame)
gdf_point = gpd.GeoDataFrame(df, 
      geometry=gpd.points_from_xy(df.longitude, df.latitude, crs='epsg:4326'))
# make sure that gdf_point and gdf_bor have the same projected CRS
gdf_point=gdf_point.to_crs('epsg:27700')
gdf_bor=gdf_bor.to_crs('epsg:27700')

# Creates a new figure with specified number of
# subplots (we'll see more of this later) and 
# and the specified size (in inches by default).

import matplotlib.pyplot as plt
import geopandas as gpd

# 创建子图
fig, ax = plt.subplots(2, 2, figsize=(14, 13))

# 绘制底图
gdf_bor.plot(ax=ax[0,0], edgecolor='red', facecolor='white')
gdf_bor.plot(ax=ax[0,1], edgecolor='red', facecolor='white')
gdf_bor.plot(ax=ax[1,0], edgecolor='red', facecolor='white')

# 添加矢量点数据
gdf_point[gdf_point['local_or_company'] == 1].plot(ax=ax[0,0], color='green', alpha=0.3, markersize=1)
gdf_point[gdf_point['local_or_company'] == 0].plot(ax=ax[0,1], color='orange', alpha=0.3, markersize=1)
gdf_point.plot(ax=ax[1,0], color=gdf_point['local_or_company'].map({1: 'green', 0: 'orange'}), alpha=0.3 ,markersize=1)

# 添加标题和图幅标号
ax[0,0].set_title('Airbnbs of Local Hosts in London')
ax[0,0].text(-0.1, 1.1, '(a)', transform=ax[0,0].transAxes)

ax[0,1].set_title('Airbnbs of Company in London')
ax[0,1].text(-0.1, 1.1, '(b)', transform=ax[0,1].transAxes)

ax[1,0].set_title('Airbnbs in London')
ax[1,0].text(-0.1, 1.1, '(c)', transform=ax[1,0].transAxes)

# Set the x and y limits
ax[0,0].set_xlim(501000,563000)
ax[0,0].set_ylim(155000,202000)
ax[0,1].set_xlim(501000,563000)
ax[0,1].set_ylim(155000,202000)
ax[1,0].set_xlim(501000,563000)
ax[1,0].set_ylim(155000,202000)
# 添加图例、比例尺和指北针到ax[1,1]

from matplotlib.lines import Line2D
legend_elements = [Line2D([0], [0], marker='o', color='w', label='Local Hosts', markersize=5, markerfacecolor='green'),
                   Line2D([0], [0], marker='o', color='w', label='Airbnbs of Company', markersize=5, markerfacecolor='orange'), 
                   Line2D([0], [0], color='red', lw=2, linestyle='-', label='Boundary Line of the Boroughs')]
ax[1, 1].legend(handles=legend_elements, title='The Type of Airbnb')
ax[1, 1].axis('off') 

# 绘制比例尺和指北针（使用epsg:27700要麻烦一些）
# 绘制比例尺
def draw_scalebar(ax, xpos, ypos, length, unit='m', fontsize=10):
    """
    在指定的坐标轴上绘制线段比例尺。
    
    参数:
    ax -- matplotlib的坐标轴对象
    xpos, ypos -- 比例尺的起始位置（单位：米）
    length -- 比例尺代表的实际长度（单位：米）
    unit -- 单位标记（默认为米）
    fontsize -- 字体大小
    """
    # 绘制比例尺线段
    ax.plot([xpos, xpos + length], [ypos, ypos], color='black', lw=2)
    # 绘制比例尺端点的小线
    ax.plot([xpos, xpos], [ypos - 100, ypos + 100], color='black', lw=2)  # 端点线的长度为100米
    ax.plot([xpos + length, xpos + length], [ypos - 100, ypos + 100], color='black', lw=2)

    # 添加长度标注
    ax.text(xpos + length / 2, ypos - 200, f'{length} {unit}', horizontalalignment='center', fontsize=fontsize)


# 在适当的子图上绘制比例尺


def draw_north_arrow(ax, x, y, arrow_length=0.1, text='N'):
    # 绘制指向北方的箭头
    ax.annotate(text, xy=(x, y + arrow_length), xytext=(x, y),
                arrowprops=dict(facecolor='black', width=5, headwidth=15),
                ha='center', va='bottom')
# 假设 ax 是你的地图的Axes对象
draw_north_arrow(ax[1, 1], x=0.95, y=0.05)

# 设置整体标题
fig.suptitle('Airbnbs Map in London')

# 保存图形

plt.show()
```

The spatial map above shows the distributions of different types of hosts. For instance, it can be viewed that most of the hosts of London Airbnb are local. In addition, the distribution of Local hosts and company host are similar which is they are gathering near the center of London.

```{python}
import pandas as pd

def summarize_room_types():
    # Load the CSV file directly using the file name
    data = pd.read_csv('../data/listings.csv')

    # Calculate the count and percentage of each room type
    room_type_summary = data['room_type'].value_counts().to_frame()
    room_type_summary.columns = ['Count']
    room_type_summary['Percentage'] = (room_type_summary['Count'] / room_type_summary['Count'].sum()) * 100

    return room_type_summary

# Generate the summary and print it
summary = summarize_room_types()
print(summary)
```

The table above shows the percentage of different types of Airbnb in London. It can be observed that the type of "Entire home" owns the highest proportion. As a result, as indicated by several researches, the decrease of long-term rental markets are decreased and the rental prices increase. To reduce the impact of short-term rental to long-term rental, a 90 days limited policy was published which serves to prevent long-term rental properties from being turned into year-round vacation rentals and keeps them accessible for locals.

## 7. Drawing on your previous answers, and supporting your response with evidence (e.g. figures, maps, and statistical analysis/models), how *could* this data set be used to inform the regulation of Short-Term Lets (STL) in London? 

In our study, we wanted to determine the influence of Airbnb to non-tourism areas. Therefore, the local economy is calculated which contains tourist expenditure and landlord feedback economy is determined in the following trunks. An "Occupancy rate" model carried out from research made by the Budget and Legislative Analyst’s Office in San Francisco using the data from Inside Airbnb is mainly used.

### 7.1 Tourist expenditure calculation

```{python}
# Assuming DataFrame is named 'df'
# Replace 'df' with the actual name of your DataFrame variable

import re
```

The review rate and the average stay nights are set to be 50% and 3 days respectively<sup>1</sup>.

```{python}
# Setting the model parameters
review_rate = 0.5
average_days = 3
```

Furthermore, the trunk below first calculated the quantity of tourists of each Airbnb annually (assumption had been made that one bed for one person) with the "Occupancy rate" model and then multiply with a spend coefficient (43 pounds) which carried out by a research by Airbnb<sup>4</sup> "The average Airbnb visitor in the UK spends £100 a day, of which 43% is spent in the neighborhood where they are staying."

```{python}
# Selecting listings where availability_365 is greater than 0
df_geq_0 = df[df.availability_365 > 0]

df_other = df

df_other = df_other.drop('neighbourhood_group', axis=1)
# Calculating estimated annual nights

condition = df.availability_365 > 0
estimated_nights = (
    (df_geq_0.reviews_per_month / review_rate * average_days * 12) / 365
) * df_geq_0.availability_365
df_other.loc[condition, 'estimated_annual_nights'] = estimated_nights

#df_other.loc[df.availability_365 > 0, 'estimated_annual_nights'] = (
#    (df_geq_0.reviews_per_month / review_rate * average_days * 12) / 365
#) * df_geq_0.availability_365


# Setting estimated annual income to 0 for listings with availability_365 equals 0
df_other.loc[df.availability_365 == 0, 'estimated_annual_income'] = 0

# Calculating estimated annual income
df_other.loc[df.availability_365 > 0, 'estimated_annual_income'] = df_other['estimated_annual_nights'] * df_geq_0.price

# Filling any missing values in both new columns with 0
df_other[['estimated_annual_nights', 'estimated_annual_income']] = df_other[['estimated_annual_nights', 'estimated_annual_income']].fillna(0)

#df_other['local_or_company'].max()

# Extract the number of beds from the 'name' column.
def extract_beds(name):
    match = re.search(r'(\d+)\s+beds?', name)
    return int(match.group(1)) if match else 0

df_other['number_of_beds'] = df['name'].apply(extract_beds)

# Calculate 'tourists_expenditure'
df_other['tourists_expenditure'] = df_other['number_of_beds'] * df_other['estimated_annual_nights'] * 100
#df_other.head(4)
```

### 7.2 Local economic multiplier calculation

A LM3 model is applied which assumed that When 1 pound is spent with a local vendor, the local economy benefits by 1.76 pounds, but only by 36 pence when it is spent elsewhere<sup>5</sup>.

```{python}
# applying local economic multiplier
# Model coefficient
coef1 = 1.76
coef2 = 0.36

# local landlords using 1.76, real estate company landlord using 0.36
df_other.loc[df_other.local_or_company==1,'value_to_local_economy']=df_other.loc[df_other.local_or_company==1,'estimated_annual_income']*coef1
df_other.loc[df_other.local_or_company==0,'value_to_local_economy']=df_other.loc[df_other.local_or_company==0,'estimated_annual_income']*coef2

# Update 'value_to_local_economy' by adding the product of 'tourists_expenditure' and 0.43)
df_other['value_to_local_economy'] = df_other['tourists_expenditure'] * 0.43 + df_other['value_to_local_economy']
```

```{python}
df_other = df_other.groupby('neighbourhood')['value_to_local_economy'].sum().reset_index(name='local_economy_contribution_from_airbnb')
df_other['local_economy_contribution_from_airbnb'] = df_other['local_economy_contribution_from_airbnb'].round(2)

# df_other.head(4)
# Generate a new CSV file to display the local economy contribution from Airbnb tourism for each borough.
# df.to_csv('local_economy_contribution_from_airbnb_tourism.csv', index=False)
```

### 7.3 Cluster for tourism borough and non-tourism borough

Spatial data of London's attractions were acquired using OSM and mapped on the boundaries of London borough. The number of attractions contained in each borough was analyzed by clustering to distinguish non-tourist boroughs. As shown in the Figure，Lambeth, Southwark, Richmond upon Thames, Kensington and Chelsea, Westminster, Camden, and Tower Hamlets own more tourist attracions，therefore they are classified as tourist area and the other boroughs are classified as non-tourist areas. The later analysis mainly focus on the impact of airbnb visitors to non-tourist areas.

```{python}
# package
import json
import pandas as pd
from shapely.geometry import Point
import geopandas as gpd
import pandas as pdpip
```

```{python}
## Read London borough boundary 

shapefile_path = '../data/London_Borough_Excluding_MHW/London_Borough_Excluding_MHW.shp'

file_path = '../data/export.geojson'

# load GeoJSON file
with open(file_path, 'r', encoding='utf-8') as file:
    geojson_data = json.load(file)
    
# Create a list to store coordinate
point_coordinates = []

# Iterate over each feature in the feature set
for feature in geojson_data.get('features', []):
    # Check whether the geometry type is a point
    if feature['geometry']['type'] == 'Point':
        # Extract coordinates
        coordinates = feature['geometry']['coordinates']
        point_coordinates.append(coordinates)
```

```{python}
points = [Point(x, y) for x, y in point_coordinates]

# create DataFrame
df = pd.DataFrame(point_coordinates, columns=['Longitude', 'Latitude'])

# use DataFrame and Points create GeoDataFrame
attraction_gdf = gpd.GeoDataFrame(df, geometry=points)
```

```{python}
# load Shapefile
borough = gpd.read_file(shapefile_path)
attraction_gdf.set_crs(epsg=4326, inplace=True)

# Ensure use the same cr
attraction_gdf = attraction_gdf.to_crs(borough.crs)
```

```{python}
# Calculate the number of attraction points contained in each borough


# Use spatial joins to count the number of points each borough contains
point_in_polygon = gpd.sjoin(attraction_gdf, borough, how='inner', op='within')

# Calculate the number of points contained in each borough
count_points_in_polygon = point_in_polygon.groupby('GSS_CODE').size().reset_index(name='point_count')

# spatial join
point_in_polygon = gpd.sjoin(attraction_gdf, borough, how='inner', op='within')

# groupby borough and count
count_points = point_in_polygon.groupby('GSS_CODE').size()

# Converts the count result to a DataFrame and resets the index
count_points_df = count_points.reset_index(name='point_count')

# Ensure that all polygons are included in the statistics
count_points_in_polygon = borough.merge(count_points_df, on='GSS_CODE', how='left')

# Sets the number of points for polygons that do not contain points to zero
count_points_in_polygon['point_count'].fillna(0, inplace=True)

# print
# print(count_points_in_polygon[['GSS_CODE', 'point_count']])
```

```{python}
# Attraction count cluster analysis


# Extract the point count as the clustering feature
X = count_points_in_polygon[['point_count']].values

n_clusters = 3

kmeans = KMeans(n_clusters=n_clusters, random_state=44)
kmeans.fit(X)

# Adds the cluster tag to the original DataFrame
count_points_in_polygon['cluster'] = kmeans.labels_

# Find the index with the smallest point count
min_point_count_cluster = kmeans.labels_[np.argmin(X)]

# Reassign the cluster label so that the cluster of minimum point_count is 1
count_points_in_polygon['cluster'] = [(label + 1 if label >= min_point_count_cluster 
                                       else label) for label in count_points_in_polygon['cluster']]
count_points_in_polygon['cluster'] = count_points_in_polygon['cluster'].replace(min_point_count_cluster, 1)
```

```{python}
# Borough Attraction Count visualization

# Use the value of the 'point count' column to set the color map
count_points_in_polygon.plot(column='point_count', cmap='plasma_r', legend=True, figsize=(10, 6))

# Add titles and labels
plt.title('Borough Attraction Count')
plt.xlabel('X Coordinate')
plt.ylabel('Y Coordinate')


plt.show()

# Cluster plot

# Set color: cluster equals 1 for one color and the rest for another color
colors = count_points_in_polygon['cluster'].map(lambda x: 'silver' if x == 1 else 'grey')


count_points_in_polygon.plot(color=colors, figsize=(10, 6))

plt.xlabel('X Coordinate')
plt.ylabel('Y Coordinate')
plt.scatter([], [], color='silver', label='non tourism borough')
plt.scatter([], [], color='grey', label='tourism borough')
plt.legend(title='Clusters')


plt.show()
```

```{python}
# Select non tourism boroughs

non_tourism_borough = count_points_in_polygon[count_points_in_polygon['cluster'] == 1]
```

In conclusion, the graph above shows the tourism borough and non-tourism borough.

### 7.4 Local economy plotting

The following part uses graphs to show the local economy which brings by the Airbnb to London goroughs.

```{python}
expenditure_point = pd.read_csv('../data/updated_data.csv')
selected_expenditure_point_columns = expenditure_point[['id','latitude', 'longitude', 'tourists_expenditure']]
selected_expenditure_point_columns['latitude'] = pd.to_numeric(selected_expenditure_point_columns['latitude'], errors='coerce')
selected_expenditure_point_columns['longitude'] = pd.to_numeric(selected_expenditure_point_columns['longitude'], errors='coerce')

selected_expenditure_point_columns.dropna(inplace=True)
# convert lat/long to GeoSeries (converted the InsideAirbnb data to a GeoDataFrame)
expenditure_gdf = gpd.GeoDataFrame(selected_expenditure_point_columns, 
      geometry=gpd.points_from_xy(selected_expenditure_point_columns.longitude, selected_expenditure_point_columns.latitude, crs='epsg:4326'))

# plot and reproject it

expenditure_gdf = expenditure_gdf.to_crs('epsg:27700')
print(expenditure_gdf.crs)

expenditure_non = gpd.sjoin(expenditure_gdf, non_tourism_borough, predicate='within')
# 确保两个 GeoDataFrame 使用相同的坐标参考系统
expenditure_non = expenditure_non.to_crs(non_tourism_borough.crs)

grouped_data = expenditure_non.groupby('GSS_CODE')['tourists_expenditure'].sum().reset_index()
merged_data = borough.merge(grouped_data, on='GSS_CODE')
merged_data.plot(column='tourists_expenditure', cmap='plasma_r', legend=True, figsize=(10, 6))

# 添加标题和标签（可选）
plt.title('Tourists Expenditure in Each Borough')
plt.xlabel('Longitude')
plt.ylabel('Latitude')

# 显示图表
plt.show()

# df_economy 经济列
# count_points_in_polygon

borough_result = gpd.read_file(shapefile_path)
gdf_result = gpd.GeoDataFrame(borough_result, geometry='geometry')

# df_other 
# 32
# -- neighbourhood
# gdf_result
# 32

# 使用 'id' 字段连接 DataFrame 和 GeoDataFrame
merged_gdf = gdf_result.merge(df_other, left_on='NAME', right_on = 'neighbourhood')

# 显示连接后的 GeoDataFrame
type(merged_gdf)

merged_gdf.plot(column='local_economy_contribution_from_airbnb', cmap='Blues', legend=True, figsize=(10, 6), linewidths=0.1, edgecolor='grey')
```

A spatial map is provided which shows the influence of local economy of non-tourism from Airbnb. It can be viewed that non-tourism boroughs near tourism boroughs tends to have higher rate of economy contribution rate. Therefore, it can be adviced that the 90-days limit policy can be extended for some non-tourism areas because some short-term rental Airbnb can contribute to local economy.

## Sustainable Authorship Tools

Your QMD file should automatically download your BibTeX file. We will then re-run the QMD file to generate the output successfully.

Written in Markdown and generated from [Quarto](https://quarto.org/). Fonts used: [Spectral](https://fonts.google.com/specimen/Spectral) (mainfont), [Roboto](https://fonts.google.com/specimen/Roboto) (<span style="font-family:Sans-Serif;">sansfont</span>) and [JetBrains Mono](https://fonts.google.com/specimen/JetBrains%20Mono) (`monofont`). 

## References

1. Inside Airbnb. Available at: http://insideairbnb.com/get-the-data/ (Accessed: 18 December 2023). 

```{python}
2. 
```

3. Abdulkareem Alsudais (2021) ‘Incorrect data in the widely used Inside Airbnb dataset’.

4. (No date a) Airbnb UK insights report - airbnb citizen. Available at: https://www.airbnbcitizen.com/wp-content/uploads/2018/10/AirbnbUKInsightsReport_2018.pdf (Accessed: 19 December 2023). 

5. About LM3 (no date) LM3 Online. Available at: https://www.lm3online.com/about (Accessed: 19 December 2023). 
